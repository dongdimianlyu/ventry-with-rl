import os
import json
import numpy as np
from datetime import datetime
from typing import Dict, Any, Optional, List, Tuple
from stable_baselines3 import PPO, SAC, A2C
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.vec_env import VecNormalize
from enhanced_business_env import EnhancedBusinessEnv
from restocking_env import RestockingEnv
import warnings
warnings.filterwarnings('ignore')


class EnhancedRestockingAgent:
    """
    Enhanced Reinforcement Learning agent for complex SME business decisions.
    
    Features:
    - Support for multiple RL algorithms (PPO, SAC, A2C)
    - Advanced training strategies including curriculum learning
    - Improved reward function with business-relevant metrics
    - Better exploration through advanced hyperparameters
    - Offline RL training from historical data
    - Multi-objective optimization
    - Enhanced recommendation generation
    """
    
    def __init__(self, 
                 model_path: str = "models/enhanced_restocking_agent.zip",
                 algorithm: str = "PPO",
                 use_enhanced_env: bool = True):
        self.model_path = model_path
        self.algorithm = algorithm
        self.use_enhanced_env = use_enhanced_env
        self.model = None
        self.env = None
        self.vec_env = None
        
        self.training_stats = {
            "episodes_trained": 0,
            "total_timesteps": 0,
            "best_reward": -float('inf'),
            "training_history": [],
            "curriculum_stage": 0,
            "algorithm_used": algorithm
        }
        
        # Create models directory if it doesn't exist
        os.makedirs(os.path.dirname(model_path), exist_ok=True)
        
    def create_environment(self, n_envs: int = 1, normalize: bool = True) -> None:
        """Create the training environment with enhanced features"""
        env_class = EnhancedBusinessEnv if self.use_enhanced_env else None
        
        if env_class is None:
            # Fallback to original environment if enhanced is not available
            from restocking_env import RestockingEnv
            env_class = RestockingEnv
            
        if n_envs == 1:
            self.env = Monitor(env_class())
        else:
            # Create vectorized environment for faster training
            self.env = make_vec_env(env_class, n_envs=n_envs)
            
            if normalize and self.use_enhanced_env:
                # Normalize observations and rewards for better training
                self.env = VecNormalize(self.env, norm_obs=True, norm_reward=True)
                self.vec_env = self.env
    
    def create_model(self, 
                     learning_rate: float = 3e-4, 
                     verbose: int = 1,
                     advanced_config: bool = True) -> None:
        """Create an RL model with advanced configuration"""
        if self.env is None:
            self.create_environment()
        
        if self.env is None:
            raise RuntimeError("Failed to create environment")
        
        print(f"Creating {self.algorithm} model with {'advanced' if advanced_config else 'standard'} configuration...")
        
        if self.algorithm == "PPO":
            if advanced_config:
                # Advanced PPO configuration for complex business environments
                self.model = PPO(
                    "MlpPolicy",
                    self.env,
                    learning_rate=learning_rate,
                    n_steps=4096,  # Longer rollouts for better learning
                    batch_size=128,  # Larger batch size
                    n_epochs=20,  # More epochs for better convergence
                    gamma=0.995,  # Higher discount factor for long-term planning
                    gae_lambda=0.98,  # Better advantage estimation
                    clip_range=0.2,
                    clip_range_vf=None,
                    normalize_advantage=True,
                    ent_coef=0.01,  # Encourage exploration
                    vf_coef=0.5,
                    max_grad_norm=0.5,
                    target_kl=0.02,  # Better policy updates
                    verbose=verbose,
                    tensorboard_log="./tensorboard_logs/",
                    policy_kwargs=dict(
                        net_arch=[256, 256, 128],  # Larger network
                        activation_fn="tanh"
                    )
                )
            else:
                # Standard PPO configuration
                self.model = PPO(
                    "MlpPolicy",
                    self.env,
                    learning_rate=learning_rate,
                    n_steps=2048,
                    batch_size=64,
                    n_epochs=10,
                    gamma=0.99,
                    gae_lambda=0.95,
                    clip_range=0.2,
                    verbose=verbose,
                    tensorboard_log="./tensorboard_logs/"
                )
        
        elif self.algorithm == "SAC":
            # SAC for continuous control aspects
            self.model = SAC(
                "MlpPolicy",
                self.env,
                learning_rate=learning_rate,
                buffer_size=100000,
                learning_starts=1000,
                batch_size=256,
                tau=0.005,
                gamma=0.995,
                train_freq=1,
                gradient_steps=1,
                ent_coef='auto',
                target_update_interval=1,
                verbose=verbose,
                tensorboard_log="./tensorboard_logs/",
                policy_kwargs=dict(net_arch=[256, 256])
            )
        
        elif self.algorithm == "A2C":
            # A2C for simpler, faster training
            self.model = A2C(
                "MlpPolicy",
                self.env,
                learning_rate=learning_rate,
                n_steps=20,
                gamma=0.99,
                gae_lambda=0.95,
                ent_coef=0.01,
                vf_coef=0.5,
                max_grad_norm=0.5,
                verbose=verbose,
                tensorboard_log="./tensorboard_logs/",
                policy_kwargs=dict(net_arch=[128, 128])
            )
        
        else:
            raise ValueError(f"Unsupported algorithm: {self.algorithm}")
    
    def train_curriculum(self, 
                        total_timesteps: int = 200000,
                        save_freq: int = 25000) -> None:
        """Train using curriculum learning approach"""
        if self.model is None:
            self.create_model()
        
        print(f"Starting curriculum training for {total_timesteps} timesteps...")
        
        # Define curriculum stages
        curriculum_stages = [
            {"name": "Basic Operations", "timesteps": total_timesteps // 4, "difficulty": 0.5},
            {"name": "Market Volatility", "timesteps": total_timesteps // 4, "difficulty": 0.7},
            {"name": "Complex Scenarios", "timesteps": total_timesteps // 4, "difficulty": 0.9},
            {"name": "Full Complexity", "timesteps": total_timesteps // 4, "difficulty": 1.0}
        ]
        
        total_trained = 0
        
        for stage_idx, stage in enumerate(curriculum_stages):
            print(f"\nðŸŽ“ Curriculum Stage {stage_idx + 1}: {stage['name']}")
            print(f"   Difficulty: {stage['difficulty']}, Timesteps: {stage['timesteps']}")
            
            self.training_stats["curriculum_stage"] = stage_idx + 1
            
            # Create evaluation environment
            eval_env = Monitor(EnhancedBusinessEnv() if self.use_enhanced_env else RestockingEnv())
            
            eval_callback = EvalCallback(
                eval_env,
                best_model_save_path=f"./models/stage_{stage_idx}/",
                log_path="./logs/",
                eval_freq=save_freq,
                deterministic=True,
                render=False,
                verbose=1
            )
            
            # Train for this stage
            self.model.learn(
                total_timesteps=stage['timesteps'],
                callback=eval_callback,
                progress_bar=True,
                reset_num_timesteps=False
            )
            
            total_trained += stage['timesteps']
            
            # Update training stats
            self.training_stats["total_timesteps"] = total_trained
            self.training_stats["episodes_trained"] += stage['timesteps'] // 90
            
            print(f"   âœ… Stage {stage_idx + 1} completed. Total trained: {total_trained}")
        
        # Save the final model
        self.save_model()
        print(f"\nðŸ† Curriculum training completed! Total timesteps: {total_trained}")
    
    def train_standard(self, 
                      total_timesteps: int = 100000, 
                      save_freq: int = 20000) -> None:
        """Standard training without curriculum"""
        if self.model is None:
            self.create_model()
        
        print(f"Starting standard training for {total_timesteps} timesteps...")
        
        # Create evaluation environment
        eval_env_class = EnhancedBusinessEnv if self.use_enhanced_env else RestockingEnv
        eval_env = Monitor(eval_env_class())
        
        eval_callback = EvalCallback(
            eval_env,
            best_model_save_path="./models/",
            log_path="./logs/",
            eval_freq=save_freq,
            deterministic=True,
            render=False,
            verbose=1
        )
        
        # Train the model
        self.model.learn(
            total_timesteps=total_timesteps,
            callback=eval_callback,
            progress_bar=True
        )
        
        # Save the final model
        self.save_model()
        
        # Update training stats
        self.training_stats["total_timesteps"] += total_timesteps
        self.training_stats["episodes_trained"] += total_timesteps // 90
        
        print(f"Training completed! Model saved to {self.model_path}")
    
    def train_from_demonstrations(self, 
                                demonstration_data: List[Dict],
                                total_timesteps: int = 50000) -> None:
        """Train using expert demonstrations (offline RL)"""
        print("Training from demonstrations not yet implemented for this environment.")
        print("Falling back to standard training...")
        self.train_standard(total_timesteps)
    
    def load_model(self) -> bool:
        """Load a pre-trained model"""
        try:
            if os.path.exists(self.model_path):
                if self.algorithm == "PPO":
                    self.model = PPO.load(self.model_path)
                elif self.algorithm == "SAC":
                    self.model = SAC.load(self.model_path)
                elif self.algorithm == "A2C":
                    self.model = A2C.load(self.model_path)
                
                print(f"Model loaded from {self.model_path}")
                return True
            else:
                print(f"No model found at {self.model_path}")
                return False
        except Exception as e:
            print(f"Error loading model: {e}")
            return False
    
    def save_model(self) -> None:
        """Save the current model"""
        if self.model is not None:
            self.model.save(self.model_path)
            
            # Also save training stats
            stats_path = self.model_path.replace('.zip', '_stats.json')
            with open(stats_path, 'w') as f:
                json.dump(self.training_stats, f, indent=2)
            
            print(f"Model saved to {self.model_path}")
    
    def predict(self, observation: np.ndarray, deterministic: bool = True) -> np.ndarray:
        """Make a prediction given an observation"""
        if self.model is None:
            raise ValueError("Model not loaded or trained. Call load_model() or train() first.")
        
        action, _ = self.model.predict(observation, deterministic=deterministic)
        return action
    
    def generate_enhanced_recommendations(self, n_episodes: int = 10) -> Dict[str, Any]:
        """Generate enhanced business recommendations using the trained agent"""
        if self.model is None:
            if not self.load_model():
                raise ValueError("No trained model available. Train the model first.")
        
        # Create test environment
        test_env = EnhancedBusinessEnv() if self.use_enhanced_env else RestockingEnv()
        
        all_recommendations = []
        episode_metrics = []
        
        print(f"Generating enhanced recommendations from {n_episodes} episodes...")
        
        for episode in range(n_episodes):
            obs, info = test_env.reset()
            episode_profit = 0
            episode_revenue = 0
            episode_actions = []
            customer_satisfaction_history = []
            
            for step in range(90 if self.use_enhanced_env else 30):
                action = self.predict(obs, deterministic=True)
                obs, reward, terminated, truncated, info = test_env.step(action)
                
                episode_profit += info.get("total_profit", 0)
                episode_revenue += info.get("monthly_revenue", 0)
                
                if self.use_enhanced_env:
                    customer_satisfaction_history.append(info.get("customer_satisfaction", 0.8))
                
                # Record significant actions
                if hasattr(info, 'episode_actions') and info['episode_actions']:
                    for action_info in info['episode_actions']:
                        if action_info not in episode_actions:  # Avoid duplicates
                            episode_actions.append(action_info)
                
                if terminated or truncated:
                    break
            
            # Calculate episode metrics
            avg_satisfaction = np.mean(customer_satisfaction_history) if customer_satisfaction_history else 0.8
            
            episode_metrics.append({
                "profit": episode_profit,
                "revenue": episode_revenue,
                "customer_satisfaction": avg_satisfaction,
                "actions_taken": len(episode_actions)
            })
            
            all_recommendations.extend(episode_actions)
        
        # Analyze results
        avg_profit = np.mean([m["profit"] for m in episode_metrics])
        avg_revenue = np.mean([m["revenue"] for m in episode_metrics])
        avg_satisfaction = np.mean([m["customer_satisfaction"] for m in episode_metrics])
        
        # Find best recommendations by ROI
        if all_recommendations:
            best_recommendations = sorted(
                all_recommendations, 
                key=lambda x: float(x.get("expected_roi", "0%").replace('%', '')), 
                reverse=True
            )[:5]
        else:
            best_recommendations = []
        
        # Generate comprehensive recommendation
        if best_recommendations:
            primary_rec = best_recommendations[0]
            action = primary_rec.get("action", "monitor")
            product = primary_rec.get("product", "inventory")
            quantity = primary_rec.get("quantity", 0)
            expected_roi = primary_rec.get("expected_roi", "0%")
            confidence = self._calculate_confidence(episode_metrics, best_recommendations)
        else:
            action = "monitor"
            product = "business operations"
            quantity = 0
            expected_roi = "0%"
            confidence = "low"
        
        # Generate sophisticated reasoning
        reasoning = self._generate_enhanced_reasoning(
            episode_metrics, best_recommendations, float(avg_satisfaction), float(avg_revenue)
        )
        
        return {
            "action": action,
            "product": product,
            "quantity": quantity,
            "expected_roi": expected_roi,
            "confidence": confidence,
            "reasoning": reasoning,
            "timestamp": datetime.now().isoformat(),
            "algorithm": self.algorithm,
            "episodes_analyzed": n_episodes,
            "business_metrics": {
                "average_profit": f"${avg_profit:.2f}",
                "average_revenue": f"${avg_revenue:.2f}",
                "customer_satisfaction": f"{avg_satisfaction:.2f}",
                "simulation_quality": "high" if self.use_enhanced_env else "standard"
            },
            "alternative_actions": [
                {
                    "action": rec.get("action", "unknown"),
                    "product": rec.get("product", "unknown"),
                    "quantity": rec.get("quantity", 0),
                    "expected_roi": rec.get("expected_roi", "0%")
                }
                for rec in best_recommendations[1:4]
            ]
        }
    
    def _calculate_confidence(self, episode_metrics: List[Dict], recommendations: List[Dict]) -> str:
        """Calculate confidence level based on episode consistency"""
        if not episode_metrics or not recommendations:
            return "low"
        
        profit_variance = np.var([m["profit"] for m in episode_metrics])
        avg_profit = np.mean([m["profit"] for m in episode_metrics])
        
        # Calculate confidence based on consistency and profitability
        consistency_score = 1.0 / (1.0 + profit_variance / max(abs(avg_profit), 1))
        profitability_score = min(1.0, max(0.0, avg_profit / 10000))  # Normalize around $10k profit
        
        combined_score = (consistency_score + profitability_score) / 2
        
        if combined_score > 0.8:
            return "high"
        elif combined_score > 0.5:
            return "medium"
        else:
            return "low"
    
    def _generate_enhanced_reasoning(self, 
                                   episode_metrics: List[Dict], 
                                   recommendations: List[Dict],
                                   avg_satisfaction: float,
                                   avg_revenue: float) -> str:
        """Generate sophisticated reasoning for recommendations"""
        
        if not recommendations:
            return f"After analyzing {len(episode_metrics)} business scenarios, no profitable opportunities were identified. Current operations show average customer satisfaction of {avg_satisfaction:.2f} and revenue patterns suggest maintaining current inventory levels."
        
        primary_rec = recommendations[0]
        
        # Analyze business context
        satisfaction_level = "excellent" if avg_satisfaction > 0.85 else "good" if avg_satisfaction > 0.75 else "moderate"
        revenue_level = "strong" if avg_revenue > 180000 else "adequate" if avg_revenue > 150000 else "below target"
        
        reasoning = f"Based on advanced simulation of {len(episode_metrics)} business scenarios using {self.algorithm} algorithm. "
        reasoning += f"Analysis shows {satisfaction_level} customer satisfaction ({avg_satisfaction:.2f}) and {revenue_level} revenue performance (${avg_revenue:,.0f}). "
        
        # Product-specific reasoning
        product = primary_rec.get("product", "inventory")
        action = primary_rec.get("action", "monitor")
        quantity = primary_rec.get("quantity", 0)
        expected_roi = primary_rec.get("expected_roi", "0%")
        
        if action == "restock_small" or action == "restock_medium" or action == "restock_large":
            reasoning += f"Recommending {action.replace('_', ' ')} of {quantity} units for {product}. "
            reasoning += f"This strategy is projected to yield {expected_roi} ROI based on demand patterns, "
            reasoning += f"seasonal factors, and current market conditions. "
        
        # Add market insights
        reasoning += f"The simulation considered multi-product dynamics, cash flow optimization, "
        reasoning += f"supplier relationships, and customer satisfaction metrics to ensure sustainable growth."
        
        return reasoning
    
    def evaluate_performance(self, n_episodes: int = 20) -> Dict[str, float]:
        """Enhanced performance evaluation"""
        if self.model is None:
            if not self.load_model():
                raise ValueError("No trained model available.")
        
        test_env = EnhancedBusinessEnv() if self.use_enhanced_env else RestockingEnv()
        
        episode_rewards = []
        episode_profits = []
        episode_revenues = []
        customer_satisfactions = []
        
        for episode in range(n_episodes):
            obs, info = test_env.reset()
            episode_reward = 0
            
            for step in range(90 if self.use_enhanced_env else 30):
                action = self.predict(obs, deterministic=True)
                obs, reward, terminated, truncated, info = test_env.step(action)
                episode_reward += reward
                
                if terminated or truncated:
                    break
            
            episode_rewards.append(episode_reward)
            episode_profits.append(info.get("total_profit", 0))
            
            if self.use_enhanced_env:
                episode_revenues.append(info.get("monthly_revenue", 0))
                customer_satisfactions.append(info.get("customer_satisfaction", 0.8))
        
        results = {
            "mean_reward": np.mean(episode_rewards),
            "std_reward": np.std(episode_rewards),
            "mean_profit": np.mean(episode_profits),
            "std_profit": np.std(episode_profits),
            "max_profit": np.max(episode_profits),
            "min_profit": np.min(episode_profits)
        }
        
        if self.use_enhanced_env:
            results.update({
                "mean_revenue": np.mean(episode_revenues),
                "mean_customer_satisfaction": np.mean(customer_satisfactions),
                "revenue_consistency": 1.0 - (np.std(episode_revenues) / max(np.mean(episode_revenues), 1))
            })
        
        return results 